 log.PerfLogger  -  <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
 ql.Driver  -  Concurrency mode is disabled, not creating a lock manager
 log.PerfLogger  -  <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
 parse.ParseDriver  -  Parsing command: select * from t_5 order by id 
 parse.ParseDriver - Parse Completed
 log.PerfLogger  -  </PERFLOG method=parse start=1464503172611 end=1464503172613 duration=2 from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
 parse.SemanticAnalyzer  -  Starting Semantic Analysis
 parse.SemanticAnalyzer  -  Completed phase 1 of Semantic Analysis
 parse.SemanticAnalyzer  -  Get metadata for source tables
 metastore.HiveMetaStore  -  0: get_table : db=xxo tbl=t_5
 HiveMetaStore.audit  -  ugi=root	ip=unknown-ip-addr	cmd=get_table : db=xxo tbl=t_5	
 parse.SemanticAnalyzer  -  Get metadata for subqueries
 parse.SemanticAnalyzer  -  Get metadata for destination tables
 ql.Context  -  New scratch dir is hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1
 parse.SemanticAnalyzer  -  Completed getting MetaData in Semantic Analysis
 parse.SemanticAnalyzer  -  Set stats collection dir : hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/-ext-10002
 ppd.OpProcFactory  -  Processing for FS(4)
 ppd.OpProcFactory  -  Processing for SEL(3)
 ppd.OpProcFactory  -  Processing for RS(2)
 ppd.OpProcFactory  -  Processing for SEL(1)
 ppd.OpProcFactory  -  Processing for TS(0)
 optimizer.ColumnPrunerProcFactory  -  RS 2 oldColExprMap: {VALUE._col1=Column[_col2], VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
 optimizer.ColumnPrunerProcFactory  -  RS 2 newColExprMap: {VALUE._col1=Column[_col2], VALUE._col0=Column[_col1], KEY.reducesinkkey0=Column[_col0]}
 log.PerfLogger  -  <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
 metastore.HiveMetaStore  -  0: get_partitions : db=xxo tbl=t_5
 HiveMetaStore.audit  -  ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=xxo tbl=t_5	
 log.PerfLogger  -  </PERFLOG method=partition-retrieving start=1464503172762 end=1464503172819 duration=57 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>
 physical.NullScanTaskDispatcher  -  Looking for table scans where optimization is applicable
 physical.NullScanTaskDispatcher  -  Found 0 null table scans
 physical.NullScanTaskDispatcher  -  Looking for table scans where optimization is applicable
 physical.NullScanTaskDispatcher  -  Found 0 null table scans
 physical.NullScanTaskDispatcher  -  Looking for table scans where optimization is applicable
 physical.NullScanTaskDispatcher  -  Found 0 null table scans
 parse.SemanticAnalyzer  -  Completed plan generation
 ql.Driver  -  Semantic Analysis Completed
 log.PerfLogger  -  </PERFLOG method=semanticAnalyze start=1464503172613 end=1464503172841 duration=228 from=org.apache.hadoop.hive.ql.Driver>
 exec.ListSinkOperator  -  Initializing Self OP[5]
 exec.ListSinkOperator  -  Operator 5 OP initialized
 exec.ListSinkOperator  -  Initialization Done 5 OP
 ql.Driver  -  Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:t_5.id, type:int, comment:null), FieldSchema(name:t_5.dt, type:date, comment:null), FieldSchema(name:t_5.city, type:string, comment:null)], properties:null)
 log.PerfLogger  -  </PERFLOG method=compile start=1464503172610 end=1464503172850 duration=240 from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
 ql.Driver -  Starting command: select * from t_5 order by id 
 ql.Driver  -  Query ID = root_20160529142626_a68f490b-4e9f-43c7-a3b8-d16210751de7
 ql.Driver  -  Total jobs = 1
 log.PerfLogger  -  </PERFLOG method=TimeToSubmit start=1464503172610 end=1464503172854 duration=244 from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  <PERFLOG method=task.MAPRED.Stage-1 from=org.apache.hadoop.hive.ql.Driver>
 ql.Driver  -  Launching Job 1 out of 1
 ql.Driver  - Starting task [Stage-1:MAPRED] in serial mode
 exec.Task  -  Number of reduce tasks determined at compile time: 1
 exec.Task  -  In order to change the average load for a reducer (in bytes):
 exec.Task  -  set hive.exec.reducers.bytes.per.reducer=<number>
 exec.Task  -  In order to limit the maximum number of reducers:
 exec.Task  -  set hive.exec.reducers.max=<number>
 exec.Task  -  In order to set a constant number of reducers:
 exec.Task  -  set mapreduce.job.reduces=<number>
 ql.Context  -  New scratch dir is hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1
 mr.ExecDriver  -  Using org.apache.hadoop.hive.ql.io.CombineHiveInputFormat
 exec.Utilities  -  Processing alias t_5
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=bj
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=bj
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=cq
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=cq
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=hz
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=hz
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=wz
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=wz
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=yy
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=yy
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2016-05-28/city=cq
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2016-05-28/city=cq
 exec.Utilities  -  Adding input file hdfs://xxo07:9000/user/hive/warehouse/t_1/t_1.txt
 exec.Utilities  -  Content Summary not cached for hdfs://xxo07:9000/user/hive/warehouse/t_1/t_1.txt
 exec.Utilities  -  Changed input file to hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/-mr-10003/0
 ql.Context  -  New scratch dir is hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1
 log.PerfLogger  -  <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
 exec.Utilities  -  Serializing MapWork via kryo
 log.PerfLogger  -  </PERFLOG method=serializePlan start=1464503172964 end=1464503173007 duration=43 from=org.apache.hadoop.hive.ql.exec.Utilities>
 log.PerfLogger  -  <PERFLOG method=serializePlan from=org.apache.hadoop.hive.ql.exec.Utilities>
 exec.Utilities  -  Serializing ReduceWork via kryo
 log.PerfLogger  -  </PERFLOG method=serializePlan start=1464503173015 end=1464503173148 duration=133 from=org.apache.hadoop.hive.ql.exec.Utilities>
 client.RMProxy  -  Connecting to ResourceManager at xxo07/192.168.33.72:8032
 client.RMProxy  -  Connecting to ResourceManager at xxo07/192.168.33.72:8032
 mapreduce.JobSubmitter  -  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
 log.PerfLogger  -  <PERFLOG method=getSplits from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
 io.CombineHiveInputFormat  -  CombineHiveInputSplit creating pool for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=bj; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=bj
 io.CombineHiveInputFormat  -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=cq; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=cq
 io.CombineHiveInputFormat  -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=hz; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=hz
 io.CombineHiveInputFormat  -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=wz; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=wz
 io.CombineHiveInputFormat   -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=yy; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2015-05-30/city=yy
 io.CombineHiveInputFormat  -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2016-05-28/city=cq; using filter path hdfs://xxo07:9000/user/hive/warehouse/xxo.db/t_5/dt=2016-05-28/city=cq
 io.CombineHiveInputFormat  -  CombineHiveInputSplit: pool is already created for hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/-mr-10003/0; using filter path hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/-mr-10003/0
 input.FileInputFormat  -  Total input paths to process : 7
 input.CombineFileInputFormat  -  DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
 io.CombineHiveInputFormat  -  number of splits 2
 log.PerfLogger  -  </PERFLOG method=getSplits start=1464503174738 end=1464503174933 duration=195 from=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat>
 io.CombineHiveInputFormat  -  Number of all splits 2
 mapreduce.JobSubmitter  -  number of splits:2
 mapreduce.JobSubmitter  -  Submitting tokens for job: job_1464498685344_0004
 impl.YarnClientImpl  -  Submitted application application_1464498685344_0004
 mapreduce.Job  -  The url to track the job: http://xxo07:8088/proxy/application_1464498685344_0004/
 exec.Task  -  Starting Job = job_1464498685344_0004, Tracking URL = http://xxo07:8088/proxy/application_1464498685344_0004/
 exec.Task  -  Kill Command = /usr/local/hadoop-2.6.0/bin/hadoop job  -kill job_1464498685344_0004
 exec.Task  -  Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
 mapreduce.Counters  -  Group org.apache.hadoop.mapred.Task$Counter is deprecated. Use org.apache.hadoop.mapreduce.TaskCounter instead
 exec.Task  -  2016-05-29 14:26:30,745 Stage-1 map = 0%,  reduce = 0%
 exec.Task  -  2016-05-29 14:27:53,143 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.72 sec
 exec.Task  -  2016-05-29 14:28:20,569 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 8.3 sec
 exec.Task  -  MapReduce Total cumulative CPU time: 8 seconds 300 msec
 exec.Task  -  Ended Job = job_1464498685344_0004
 exec.FileSinkOperator  -  Moving tmp dir: hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/_tmp.-ext-10001 to: hdfs://xxo07:9000/tmp/hive/root/9d2c112b-118e-46a1-8a08-c60c5bbc6fe0/hive_2016-05-29_14-26-12_611_2648166283911131981-1/-ext-10001
 log.PerfLogger  -  </PERFLOG method=runTasks start=1464503172855 end=1464503303027 duration=130172 from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  - </PERFLOG method=Driver.execute start=1464503172850 end=1464503303049 duration=130199 from=org.apache.hadoop.hive.ql.Driver>
 ql.Driver  -  MapReduce Jobs Launched: 
 ql.Driver  -  Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 8.3 sec   HDFS Read: 1028 HDFS Write: 160 SUCCESS
 ql.Driver  -  Total MapReduce CPU Time Spent: 8 seconds 300 msec
 ql.Driver  - OK
 log.PerfLogger  -  <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  </PERFLOG method=releaseLocks start=1464503303086 end=1464503303087 duration=1 from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  </PERFLOG method=Driver.run start=1464503172609 end=1464503303087 duration=130478 from=org.apache.hadoop.hive.ql.Driver>
 Configuration.deprecation  -  mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir
 mapred.FileInputFormat  -  Total input paths to process : 1
 exec.ListSinkOperator  - 5 finished. closing... 
 exec.ListSinkOperator  - 5 Close done
 CliDriver  -  Time taken: 130.479 seconds, Fetched: 10 row(s)
 log.PerfLogger  -  <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
 log.PerfLogger  -  </PERFLOG method=releaseLocks start=1464503304801 end=1464503304802 duration=1 from=org.apache.hadoop.hive.ql.Driver>
